{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Security Applications: final project\n",
    "\n",
    "This final hands on project will count for 50% of your final grade (exam is the other 50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization\n",
    "* you can start the project as of now\n",
    "* the project has to be handed in no later than March 1st - NO EXTENSIONS!\n",
    "\n",
    "### Project Submission \n",
    "* hand in a single jupyter notebook for your project (via moodle)\n",
    "* set all paths as relative paths such that the data is in the same folder as the notebook\n",
    "* your notebook should run without errors (uncomment parts that do not work)\n",
    "* Use markdown cells and comments in the code to document and motivate you solution\n",
    "* show and analyze intermediate results\n",
    "* evaluate and discuss your solution\n",
    "\n",
    "### Grading Criteria\n",
    "* 4.0 : notebook that works and gives some solution to the problem\n",
    "* 3.0 : + good documentation, evaluation and discussion\n",
    "* 2.0 : + complete processing pipeline, good results\n",
    "* 1.0 : + very detailed documentation and analysis, hyper-parameter optimization, tried and compared more than one method \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Breaking image captchas  \n",
    "\n",
    "We are working on this Kaggle Challenge: https://www.kaggle.com/fournierp/captcha-version-2-images  \n",
    "\n",
    "HINT: have a good look at the problem description and the notebooks of other users to get started \n",
    "\n",
    "* Train and test images are in the according folders\n",
    "* The true labels are encoded in the file names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks to follow:\n",
    "* write code to read the images [use the imageio lib](https://imageio.readthedocs.io/en/stable/userapi.html) and convert then into NUMPY feature vectors\n",
    "and labels\n",
    "    * HINT: use [the os lib](https://docs.python.org/2/library/os.html#os.listdir) to get files in a directory\n",
    "* the hardest part is to solve the segmentation problem: splitting the image into single characters\n",
    "    * try Clustering over pixel positions\n",
    "    * or a density projection along the y-axis\n",
    "* crate a training data set of labeled character segments\n",
    "    * evaluate this step\n",
    "* train a CNN or MLP network to classify character segments\n",
    "    * evaluate this step\n",
    "* build a full pipeline to transform capcha image inputs into strings\n",
    "* Evaluate and discuss your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/varinder/Documents/campus/sem1/DataMining2020/Project/train\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with a range of useful analytics libraries built\n",
    "# For example, here are a few handy packages to load.\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.path.join(os.getcwd(),\"train\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write code to read the images use the imageio lib and convert then into NUMPY feature vectors and labels\n",
    "HINT: use the os lib to get files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images\n",
    "images = os.listdir(os.path.join(os.getcwd(),\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating labelled data\n",
    "As we know, each of our photos includes a thread, essentially a thin line that cuts a number of characters.\n",
    "So we're going to delete the line from our picture through morphological operations (Erosion and dilation).\n",
    "First we're going to threshold our picture, we're going to use Adaptive threshold , then we're going to broaden to eliminate the line. \n",
    "It is also easily evident that each picture comprises 5 characters and the positions of all 5 characters are exactly the same.\n",
    "So we'll take the place of the hardcode character and extract five characters from each image with the corresponding name, so we'll get the labeled info. We will pad each character image in order to change it to the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel =np.ones((3,1),np.uint8)\n",
    "borderType = cv2.BORDER_CONSTANT\n",
    "def pad(src): \n",
    "    top = int(0.05 * src.shape[0])  # shape[0] = rows\n",
    "    bottom = top\n",
    "    left = int(0.15 * src.shape[1])  # shape[1] = cols\n",
    "    right = left\n",
    "    des=cv2.copyMakeBorder(src, top, bottom, left+1, right, borderType, None,255)\n",
    "    return cv2.bitwise_not(des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING LABELLED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for image in images:\n",
    "    im=cv2.imread(os.path.join(os.path.join(os.getcwd(),\"train\"),image),0)\n",
    "    threshold=cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 199, 5)\n",
    "    erosion =cv2.dilate(threshold,kernel,iterations=2)\n",
    "    s=str(image)\n",
    "    for i in range(5):\n",
    "        x.append(pad(erosion[:,(30+23*i):(30+23*(i+1))]))\n",
    "        y.append(s[-9+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa185e0ee50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)\n",
    "y=np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see which characters are present and their associated frequency, to ensure that our data is not biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2' '246']\n",
      " ['3' '240']\n",
      " ['4' '264']\n",
      " ['5' '270']\n",
      " ['6' '248']\n",
      " ['7' '236']\n",
      " ['8' '254']\n",
      " ['b' '218']\n",
      " ['c' '252']\n",
      " ['d' '242']\n",
      " ['e' '219']\n",
      " ['f' '247']\n",
      " ['g' '245']\n",
      " ['m' '247']\n",
      " ['n' '497']\n",
      " ['p' '233']\n",
      " ['w' '220']\n",
      " ['x' '252']\n",
      " ['y' '220']]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.reshape(x,(-1,54,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "l=['2','3','4','5','6','7','8','b','c','d','e','f','g','m','n','p','w','x','y']\n",
    "for j in y:\n",
    "    i=l.index(j)\n",
    "    a=[]\n",
    "    for r in range(19):\n",
    "        if(r==i):\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    a=np.array(a)\n",
    "    train.append(a)\n",
    "train=np.array(train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries to describe our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining architecture of our Model\n",
    "train a CNN or MLP network to classify character segments evaluate this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (50,30,1)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(19, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30 # Turn epochs to 30 to get 0.9711 accuracy\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, train, test_size = 0.1, random_state= 2)\n",
    "X_train=X_train/255.0\n",
    "X_val=X_val/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = False, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    \n",
    "     height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varinder/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - ETA: 0s - loss: 2.8333 - accuracy: 0.1193WARNING:tensorflow:Model was constructed with shape (None, 50, 30, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50, 30, 1), dtype=tf.float32, name='conv2d_input'), name='conv2d_input', description=\"created by layer 'conv2d_input'\"), but it was called on an input with incompatible shape (None, 54, 30, 1).\n",
      "50/50 [==============================] - 39s 745ms/step - loss: 2.8297 - accuracy: 0.1205 - val_loss: 1.6866 - val_accuracy: 0.6082\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 36s 728ms/step - loss: 2.0081 - accuracy: 0.3928 - val_loss: 0.8408 - val_accuracy: 0.7691\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 37s 730ms/step - loss: 1.4221 - accuracy: 0.5794 - val_loss: 0.5277 - val_accuracy: 0.8701\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 37s 733ms/step - loss: 1.0179 - accuracy: 0.7055 - val_loss: 0.4130 - val_accuracy: 0.8804\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 37s 731ms/step - loss: 0.7856 - accuracy: 0.7691 - val_loss: 0.3855 - val_accuracy: 0.8907\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 37s 732ms/step - loss: 0.6228 - accuracy: 0.8171 - val_loss: 0.3335 - val_accuracy: 0.9010\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 36s 720ms/step - loss: 0.5335 - accuracy: 0.8458 - val_loss: 0.2775 - val_accuracy: 0.9175\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 37s 736ms/step - loss: 0.4854 - accuracy: 0.8517 - val_loss: 0.2696 - val_accuracy: 0.9258\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 36s 722ms/step - loss: 0.4327 - accuracy: 0.8696 - val_loss: 0.2367 - val_accuracy: 0.9361\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 37s 734ms/step - loss: 0.3610 - accuracy: 0.8925 - val_loss: 0.2844 - val_accuracy: 0.9237\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 36s 724ms/step - loss: 0.3464 - accuracy: 0.8988 - val_loss: 0.2163 - val_accuracy: 0.9340\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 37s 730ms/step - loss: 0.3182 - accuracy: 0.9048 - val_loss: 0.2048 - val_accuracy: 0.9402\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 37s 742ms/step - loss: 0.2970 - accuracy: 0.9054 - val_loss: 0.2137 - val_accuracy: 0.9423\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 37s 749ms/step - loss: 0.2583 - accuracy: 0.9126 - val_loss: 0.2168 - val_accuracy: 0.9361\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 36s 721ms/step - loss: 0.2628 - accuracy: 0.9160 - val_loss: 0.1870 - val_accuracy: 0.9443\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 37s 739ms/step - loss: 0.2491 - accuracy: 0.9249 - val_loss: 0.1940 - val_accuracy: 0.9526\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 36s 722ms/step - loss: 0.2706 - accuracy: 0.9208 - val_loss: 0.1980 - val_accuracy: 0.9464\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 37s 736ms/step - loss: 0.2107 - accuracy: 0.9336 - val_loss: 0.1953 - val_accuracy: 0.9588\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 36s 727ms/step - loss: 0.2014 - accuracy: 0.9305 - val_loss: 0.1890 - val_accuracy: 0.9526\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 39s 773ms/step - loss: 0.1702 - accuracy: 0.9506 - val_loss: 0.1864 - val_accuracy: 0.9629\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 37s 733ms/step - loss: 0.2037 - accuracy: 0.9342 - val_loss: 0.1830 - val_accuracy: 0.9649\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 36s 729ms/step - loss: 0.1916 - accuracy: 0.9474 - val_loss: 0.2253 - val_accuracy: 0.9567\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 37s 738ms/step - loss: 0.1693 - accuracy: 0.9419 - val_loss: 0.1822 - val_accuracy: 0.9608\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 36s 723ms/step - loss: 0.1755 - accuracy: 0.9447 - val_loss: 0.1929 - val_accuracy: 0.9608\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 37s 737ms/step - loss: 0.1592 - accuracy: 0.9562 - val_loss: 0.1671 - val_accuracy: 0.9649\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 37s 739ms/step - loss: 0.1497 - accuracy: 0.9578 - val_loss: 0.1579 - val_accuracy: 0.9670\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 38s 762ms/step - loss: 0.1288 - accuracy: 0.9610 - val_loss: 0.1566 - val_accuracy: 0.9670\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 43s 866ms/step - loss: 0.1161 - accuracy: 0.9621 - val_loss: 0.1700 - val_accuracy: 0.9670\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 40s 796ms/step - loss: 0.1257 - accuracy: 0.9614 - val_loss: 0.1309 - val_accuracy: 0.9711\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 44s 889ms/step - loss: 0.1389 - accuracy: 0.9584 - val_loss: 0.1529 - val_accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs =  30, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING LABELLED DATA TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "for image in os.listdir(os.path.join(os.getcwd(),\"test\")):\n",
    "    im=cv2.imread(os.path.join(os.path.join(os.getcwd(),\"test\"),image),0)\n",
    "    threshold=cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 199, 5)\n",
    "    erosion =cv2.dilate(threshold,kernel,iterations=2)\n",
    "    s=str(image)\n",
    "    for i in range(5):\n",
    "        x_test.append(pad(erosion[:,(30+23*i):(30+23*(i+1))]))\n",
    "        y_test.append(s[-9+i])\n",
    "        \n",
    "        \n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "x_test=np.reshape(x_test,(-1,54,30,1))\n",
    "train_test = []\n",
    "l=['2','3','4','5','6','7','8','b','c','d','e','f','g','m','n','p','w','x','y']\n",
    "for j in y_test:\n",
    "    i=l.index(j)\n",
    "    a=[]\n",
    "    for r in range(19):\n",
    "        if(r==i):\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    a=np.array(a)\n",
    "    train_test.append(a)\n",
    "y_test=np.array(train_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and checking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 50, 30, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50, 30, 1), dtype=tf.float32, name='conv2d_input'), name='conv2d_input', description=\"created by layer 'conv2d_input'\"), but it was called on an input with incompatible shape (None, 54, 30, 1).\n",
      "16/16 [==============================] - 2s 74ms/step - loss: 16.1877 - accuracy: 0.9660\n",
      "Accuracy: [16.187732696533203, 0.9660000205039978]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test, verbose=1)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of Cpatcha Photos\n",
    "Until now, we have tested and tracked our model to a map dataset, what we are going to do here is measure the consistency of our model on captcha photos. The captcha image is said to be accurately predicted if all the characters in the image are correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captcha(img,p):\n",
    "    im=cv2.imread(str(img),0)\n",
    "    threshold=cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 199, 5)\n",
    "    erosion =cv2.dilate(threshold,kernel,iterations=2)\n",
    "    images=[]\n",
    "    for i in range(5):\n",
    "        images.append(pad(erosion[:,(30+23*i):(30+23*(i+1))]))\n",
    "    images=np.reshape(images,(-1,54,30,1))\n",
    "    pred=model.predict(images)\n",
    "    pred=np.argmax(pred,axis = 1)\n",
    "    predicted=str(l[pred[0]]+l[pred[1]]+l[pred[2]]+l[pred[3]]+l[pred[4]])\n",
    "    if(p):\n",
    "        print(predicted)\n",
    "    if(predicted==img[-9:-4]):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2g783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captcha('../Project/test/2g783.png',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and discuss your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The dilemma is a threat to recent booming advance algorithms. In the early decades, fooling computer was very difficult due to minimal resonance and analysis. We've seen this problem as an image recognition problem. There is some kind of repair restriction that we consider in the experiments to simplify our method:\n",
    "<ul><li>The number of character are fixed them and </li>\n",
    "    <li> Set the starting location of the character for splitting\n",
    "    </li></ul> </p>\n",
    "The analysis technique and our assessment are explained below.;\n",
    "<ol>\n",
    "    <li>Data preprocessing, cleaning:<br>\n",
    "        <p>I only consider the example of the sample directory to train our model. I use imageioa and opencv libraries to execute an image processing technique that sure helped to clear up a few noises. After that, i create a dataset of each character by separating it with a fixed location. The splits thus involve an unrecognizable character, which is also a constraint in our experiment.  </p>\n",
    "    </li>    \n",
    "    <li>Model desing:\n",
    "        <p> In this experiment, I use a convolutional layers neural network that is mainly appropriate for image recognition. System design optimization for the model.\n",
    "        </p>\n",
    "    </li>\n",
    "    <li>Measuring performance\n",
    "         <ol>\n",
    "         <li>Preformance on single character data:\n",
    "             <p>I assess the success only by specific character. Therefore, each character's prediction will   have better precision than the string prediction.</p>\n",
    "         </li>\n",
    "         <li>One time model evaluation:  </li>\n",
    "             <p>I train our model only once in our experiment. Therefore, different datasets are not good quality data.Any of the data is divided poorly. Thus, if the model learns from these weak input instances, success would be the worst outcome. Therefore, in order to provide a general assessment, we must attempt several runs and combine the result.At last, we could expand the amount of datasets or produce synthetic data from current datasets.</p>\n",
    "             \n",
    "         \n",
    "         \n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
