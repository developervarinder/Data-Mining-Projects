{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: DarkNet traffic detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:  Traffic Classification\n",
    "\n",
    "Kaggle challenge: https://www.kaggle.com/peterfriedrich1/cicdarknet2020-internet-traffic\n",
    "\n",
    "### CIC-Darknet2020\n",
    "\n",
    "Darknet is the unused address space of the internet which is not speculated to interact with other computers in the world. Any communication from the dark space is considered sceptical owing to its passive listening nature which accepts incoming packets, but outgoing packets are not supported. Due to the absence of legitimate hosts in the darknet, any traffic is contemplated to be unsought and is characteristically treated as probe, backscatter, or misconfiguration. Darknets are also known as network telescopes, sinkholes, or blackholes.\n",
    "\n",
    "Darknet traffic classification is significantly important to categorize real-time applications. Analyzing darknet traffic helps in early monitoring of malware before onslaught and detection of malicious activities after outbreak.\n",
    "\n",
    "\n",
    "### Data\n",
    "In CICDarknet2020 dataset, a two-layered approach is used to generate benign and darknet traffic at the first layer. The darknet traffic constitutes Audio-Stream, Browsing, Chat, Email, P2P, Transfer, Video-Stream and VOIP which is generated at the second layer. To generate the representative dataset, we amalgamated our previously generated datasets, namely, ISCXTor2016 and ISCXVPN2016, and combined the respective VPN and Tor traffic in corresponding Darknet categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Problem Statement\n",
    "Discuss the problem setting and the first implcations of the given data set... \n",
    "* What assumptions can we make about the data?\n",
    "* What problems are we expecting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: First Data Analysis, Cleaning and Feature Extraction\n",
    "* Import the data to a Pandas DataFrame\n",
    "* Run first simple statistics and visualizations\n",
    "* Is there a need to clean the data? If yes, do so...\n",
    "* Can you use the raw data directly, or should you extract features? What features are suitable ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Varinder\\datamining\\week8\\Darknet.CSV' , encoding = \"ISO-8859-1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depict all values from all the columns\n",
    "\n",
    "def display_all(data):\n",
    "    with pd.option_context(\"display.max_rows\", 800, \"display.max_columns\", 800): \n",
    "        display(data)\n",
    "display_all(data.head().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "data.isnull().sum()\n",
    "Label = pd.DataFrame(data['Label'])\n",
    "Label_1=pd.DataFrame(data['Label.1'])\n",
    "Label['Label'].nunique()\n",
    "Label_1['Label.1'].value_counts()\n",
    "data['Protocol'].nunique()\n",
    "display_all(data.dtypes.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the representation of the class variables - Label and Label.\n",
    "#Looks like the classification for class variables is unstable\n",
    "#Since there are 11 categories in the Label.1 variable, we will treat as being one of the Label.1 variable specific attributes.\n",
    "figure, ax = plt.subplots(1,2,figsize =(12,6))\n",
    "\n",
    "Label['Label'].value_counts()\n",
    "sns.countplot(x=\"Label\", palette=\"ch:.35\", ax=ax[0], data=data)\n",
    "sns.countplot(x=\"Label.1\", palette=\"ch:.65\",ax=ax[1], data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding of variables that are categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# From the results, extract categorical attributes\n",
    "\n",
    "d_1 = data.select_dtypes(include=['object']).copy()\n",
    "\n",
    "d_2 = d_1.apply(encoder.fit_transform)\n",
    "\n",
    "d_3= data.drop (['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Label', 'Label.1'], axis=1)\n",
    "\n",
    "data_encoded=pd.concat([d_3, d_2], axis=1)\n",
    "\n",
    "data_encoded['Label.1'].nunique()\n",
    "data_encoded['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict.fromkeys(data_encoded.select_dtypes(np.int32).columns, np.int64)\n",
    "data_encoded = data_encoded.astype(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(data_encoded.dtypes.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between variables\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(25,15))\n",
    "\n",
    "d_corr=data_encoded.corr()\n",
    "sns.heatmap(d_corr, cmap='rocket_r', ax=ax)\n",
    "ax.set_title(\"Matrix of Variables Correlation\", fontsize =12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropping all variables on the map that have null values\n",
    "#Variables'Flow Bytes/s 'and' Flow Packets/s have nan values - these variables are dropped' \n",
    "\n",
    "db_new=data_encoded.drop(['Fwd URG Flags', 'URG Flag Count', 'ECE Flag Count', 'Fwd Packet/Bulk Avg', 'Bwd Bytes/Bulk Avg','Subflow Bwd Packets','Active Mean', 'Active Std', 'Active Min', 'Active Max', 'Bwd PSH Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Bulk Rate Avg', 'Subflow Bwd Packets', 'Flow Bytes/s', 'Flow Packets/s'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_new\n",
    "\n",
    "#Heatmap for the latest collection of data\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(25,15))\n",
    "\n",
    "d_corr_1=db_new.corr()\n",
    "sns.heatmap(d_corr_1, cmap='rocket_r', ax=ax)\n",
    "ax.set_title(\"Matrix of Correlation variables\", fontsize =16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering only those characteristics that correlate positively or negatively with the model\n",
    "\n",
    "d_new_1 = db_new[['Protocol','Fwd Packet Length Min', 'Bwd Packet Length Min', 'Packet Length Min', 'Flow ID','Src Port', 'Dst Port', 'Flow Duration', 'Fwd Packet Length Std', 'FIN Flag Count', 'SYN Flag Count', 'Fwd Seg Size Min', 'Idle Mean', 'Idle Max', 'Timestamp', 'Label.1', 'Label']]\n",
    "\n",
    "d_new_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=d_new_1.drop(['Label'], axis=1)\n",
    "train_y=pd.DataFrame(d_new_1['Label'])\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying whether the features selected above match those created by the RF Classifier \n",
    "\n",
    "#Use RFC for selecting features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier();\n",
    "\n",
    "# fit random forest classifier on the training set\n",
    "rfc.fit(train_x,train_y);\n",
    "\n",
    "# extract Significance features\n",
    "score = np.round(rfc.feature_importances_,20)\n",
    "features = pd.DataFrame({'feature':train_x.columns,'Significance':score})\n",
    "features = features.sort_values('Significance',ascending=False).set_index('feature')\n",
    "\n",
    "# plot features\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "features.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Train a  Model\n",
    "* Which ML model would you choose and why?\n",
    "* Train and evaluate the model using the train data\n",
    "* Is the data blanced? What are the implications, how can you deal with this?\n",
    "* Discuss the results -> possible improvements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into trains and test them.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(train_x, train_y, test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier(n_neighbors=49)\n",
    "rfc = RandomForestClassifier(n_estimators=31, random_state=111)\n",
    "\n",
    "clfs = {'SVC' : svc,'KN' : knc, 'LR': lrc, 'RF': rfc}\n",
    "\n",
    "def train_classifier(clf, data_train, labels_train):    \n",
    "    clf.fit(data_train, labels_train)\n",
    "    \n",
    "def predict_labels(clf, features):\n",
    "    return (clf.predict(features))\n",
    "\n",
    "pred_scores = []\n",
    "for k,v in clfs.items():\n",
    "    train_classifier(v, data_train, labels_train)\n",
    "    pred = predict_labels(v,data_test)\n",
    "    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\n",
    "\n",
    "table = pd.DataFrame.from_items(pred_scores, orient='index', columns=['Score'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LGR = LogisticRegression(n_jobs=-1, random_state=0)\n",
    "LGR.fit(data_train, labels_train)\n",
    "LGR_predictions = LGR.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC = RandomForestClassifier(n_estimators=31, random_state=111).fit(data_train, labels_train)\n",
    "RFC_predictions = RFC.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=200).fit(data_train, labels_train)\n",
    "knn_predictions = knn.predict(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluate \n",
    "* report the F1-Score on the test data - Who will build the bes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "\n",
    "####Logistic Regression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# model accuracy for data_test   \n",
    "accuracy_LGR = LGR.score(data_test, labels_test) \n",
    "print (\"LGR-Model-Precision:\" \"\\n\", accuracy_LGR)\n",
    "\n",
    "    \n",
    "# creating a classification matrix \n",
    "cm_LGR = metrics.classification_report(labels_test, LGR_predictions)\n",
    "print(\"LGR Research on classification:\" \"\\n\", cm_LGR) \n",
    "\n",
    "\n",
    "#### RFC\n",
    "\n",
    "accuracy_RFC = RFC.score(data_test, labels_test) \n",
    "print (\"RFC_Model_Precision:\" \"\\n\", accuracy_RFC)\n",
    "    \n",
    "# creating a classification matrix \n",
    "cm_RFC = metrics.classification_report(labels_test, RFC_predictions)\n",
    "print(\"RFC_Research on classificationt:\" \"\\n\", cm_RFC) \n",
    "\n",
    "    \n",
    "#### KNN\n",
    "\n",
    "accuracy_KNN = knn.score(data_test, labels_test) \n",
    "print (\"KNN_Model_Precision:\" \"\\n\", accuracy_KNN)\n",
    "    \n",
    "# creating a classification matrix \n",
    "cm_KNN = metrics.classification_report(labels_test, knn_predictions)\n",
    "print(\"KNN_Research on classification:\" \"\\n\", cm_KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
